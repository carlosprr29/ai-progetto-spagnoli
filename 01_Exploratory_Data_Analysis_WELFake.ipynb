{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosprr29/ai-progetto-spagnoli/blob/main/01_Exploratory_Data_Analysis_WELFake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-L9fIG9G9MN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# PROJECT: Detecting Fake News with BERT\n",
        "# PHASE: Exploratory Data Analysis (EDA) - WELFake Dataset\n",
        "# =================================================================\n",
        "\n",
        "# 1. INSTALLATION AND LOADING OF LIBRARIES\n",
        "# -----------------------------------------------------------------\n",
        "print(\" Installing and loading libraries...\")\n",
        "!pip install -q datasets pandas matplotlib seaborn wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') # To clean up the output of unnecessary warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LOADING THE DATASET FROM HUGGING FACE\n",
        "# -----------------------------------------------------------------\n",
        "print(\" Loading WELFake dataset (this may take a minute)...\")\n",
        "dataset_raw = load_dataset(\"davanstrien/WELFake\")\n",
        "df = pd.DataFrame(dataset_raw['train'])\n",
        "\n",
        "# Initial cleaning: remove rows with nulls and duplicates\n",
        "df = df.dropna(subset=['title', 'text', 'label'])\n",
        "df = df.drop_duplicates()\n",
        "print(f\" Dataset successfully loaded with {len(df)} rows.\")"
      ],
      "metadata": {
        "id": "a3itH2kGHQn1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. CLASS BALANCE ANALYSIS (Bar chart)\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n Generating class balance chart...\")\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(data=df, x='label', palette='viridis')\n",
        "plt.title('News Distribution (0: Real, 1: Fake)')\n",
        "plt.xticks([0, 1], ['Real', 'Fake'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evYBccgoHU1U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. CONTENT EXPLORATION (Sampling)\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n Displaying 5 random news items from the dataset:\")\n",
        "display(df[['title', 'label']].sample(5))"
      ],
      "metadata": {
        "id": "qzqlmcIkHYAH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. TITLE LENGTH ANALYSIS\n",
        "# ----------------------------------------------------------------\n",
        "print(\"\\n Analysing title length...\")\n",
        "df['title_len'] = df['title'].astype(str).apply(len)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df, x='title_len', hue='label', kde=True, bins=100, palette='magma')\n",
        "plt.title('Comparison of Title Length: Real vs Fake')\n",
        "plt.xlim(0, 200) # We focus our attention on the most common range.\n",
        "plt.xlabel('Number of characters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xeTtFZMHa6N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. WORD CLOUD GENERATION (Term visualisation)\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n Generating word clouds (WordClouds)...\")\n",
        "\n",
        "def generate_cloud(news_class, graph_title, colour_map):\n",
        "    text = \" \".join(df[df['label'] == news_class]['title'].astype(str))\n",
        "    wc = WordCloud(width=800, height=400, background_color='white',\n",
        "                   max_words=100, colormap=colour_map).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(graph_title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Cloud for Real news (label 0)\n",
        "generate_cloud(0, \"Most common words in REAL headlines\", \"ocean\")\n",
        "\n",
        "# Cloud for Fake news (label 1)\n",
        "generate_cloud(1, \"Most common words in FAKE headlines\", \"Reds\")"
      ],
      "metadata": {
        "id": "GYrO6Hg9HgW1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. SPECIFIC KEYWORD SEARCH ENGINE\n",
        "# -----------------------------------------------------------------\n",
        "def analyse_term(term):\n",
        "    filter = df[df['title'].str.contains(term, case=False, na=False)]\n",
        "    if not filter.empty:\n",
        "        count = filter['label'].value_counts(normalize=True) * 100\n",
        "        print(f\"\\n The term '{term}' appears in {len(filter)} titles.\")\n",
        "        print(f\"   Distribution: {count.to_dict()}\")\n",
        "    else:\n",
        "        print(f\"\\n The term '{term}' was not found.\")\n",
        "\n",
        "analyse_term(\"Trump\")\n",
        "analyse_term(\"VIDEO\")\n",
        "analyse_term(\"Breaking\")\n",
        "analyse_term(\"Nasa\")"
      ],
      "metadata": {
        "id": "nhqb2Ma4HhVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XnVYc6xM7rK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}