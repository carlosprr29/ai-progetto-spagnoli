{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosprr29/ai-progetto-spagnoli/blob/main/notebooks/03_3_BERT_Training_Maestro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a third dataset; in any case, this whole process of trial and error is pure gold for documenting memory."
      ],
      "metadata": {
        "id": "VxiDPjB7e3Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have put together a 'fusion' of WELFake and ISOT (cleaning up the biases from Reuters) to see if we can achieve the definitive model, but if this is not enough, we will look for a third dataset; in any case, this whole process of trial and error is pure gold for documenting memory."
      ],
      "metadata": {
        "id": "Fjk_0MnNe5dS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5mKVUhVc5My"
      },
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# ARCHIVO 03.2: ENTRENAMIENTO DEL MODELO MAESTRO (FUSIÃ“N)\n",
        "# =================================================================\n",
        "\n",
        "# 1. INSTALLATION AND BOOKSTORES\n",
        "!pip install -q transformers datasets torch scikit-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import re\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LOADING AND CLEANING (WELFake + ISOT)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"ðŸ“¦ 1/3: Loading WELFake...\")\n",
        "ds_welfake = load_dataset(\"davanstrien/WELFake\")\n",
        "df_w = ds_welfake[\"train\"].to_pandas()[['title', 'text', 'label']].dropna()\n",
        "\n",
        "print(\"ðŸ“¦ 2/3: Loading ISOT...\")\n",
        "# Adjust the paths if your files are in another folder.\n",
        "path_true = '/content/drive/MyDrive/Project_IA/data/True.csv'\n",
        "path_fake = '/content/drive/MyDrive/Project_IA/data/Fake.csv'\n",
        "\n",
        "df_t = pd.read_csv(path_true)\n",
        "df_f = pd.read_csv(path_fake)\n",
        "df_t['label'], df_f['label'] = 0, 1\n",
        "df_i = pd.concat([df_t, df_f]).dropna()\n",
        "\n",
        "# MASTER CLEAN-UP FUNCTION (FOR BOTH DATASETS)\n",
        "def clean_master(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "\n",
        "    # A. Cleaning up datelines (Location/Agency at the beginning, such as WASHINGTON (Reuters) -)\n",
        "    text = re.sub(r'^[^-:]*[-:]\\s*', '', text)\n",
        "\n",
        "    # B. Web Noise Cleanup (URLs and Mentions)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "\n",
        "    # C. Cleaning Signatures and Agencies (Avoiding Shortcuts for Media Names)\n",
        "    # We remove mentions of Reuters, Breitbart, CNN, etc., so that BERT does not memorise brands.\n",
        "    marcas = r'\\b(Reuters|Breitbart|InfoWars|CNN|Fox News|BBC|Associated Press|AP)\\b'\n",
        "    text = re.sub(marcas, 'the news outlet', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # D. Cleaning up traces of \"Read more\" or signatures at the end\n",
        "    text = re.sub(r'(?i)read more|source\\s*[:\\-].*', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"ðŸ§¹ 3/3: Applying Unified Master Cleaning...\")\n",
        "df_w['text'] = df_w['text'].apply(clean_master)\n",
        "df_i['text'] = df_i['text'].apply(clean_master)\n",
        "\n",
        "#  MERGE AND PREPARATION\n",
        "print(\"ðŸ§ª Generating Merge and Ablation Study...\")\n",
        "df_fusion = pd.concat([df_w, df_i]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Safety filter: remove news items that have been left almost empty after cleaning\n",
        "df_fusion = df_fusion[df_fusion['text'].str.len() > 100]\n",
        "\n",
        "# Create a combined column\n",
        "df_fusion['total'] = df_fusion['title'] + \" \" + df_fusion['text']\n",
        "\n",
        "# Stratified sampling\n",
        "train_df, test_df = train_test_split(df_fusion, test_size=0.2, stratify=df_fusion['label'], random_state=42)\n",
        "train_sample = train_df.sample(n=8000, random_state=42) # We increased it to 8k due to the complexity of the merger.\n",
        "test_sample = test_df.sample(n=2000, random_state=42)"
      ],
      "metadata": {
        "id": "Q9X0JFDcdQkb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. TRAINING FUNCTION\n",
        "def train_master(x_train, x_test, y_train, y_test, name):\n",
        "    print(f\"\\nðŸš€ Training variant: {name}\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    train_enc = tokenizer(x_train.tolist(), truncation=True, padding=True, max_length=128)\n",
        "    test_enc = tokenizer(x_test.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    class DatasetTorch(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels): self.encodings, self.labels = encodings, labels\n",
        "        def __getitem__(self, idx):\n",
        "            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "        def __len__(self): return len(self.labels)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(\"cuda\")\n",
        "\n",
        "    args = TrainingArguments(output_dir=f\"./res_{name}\", num_train_epochs=2,\n",
        "                             per_device_train_batch_size=16, fp16=True,\n",
        "                             eval_strategy=\"epoch\", save_strategy=\"no\")\n",
        "\n",
        "    trainer = Trainer(model=model, args=args,\n",
        "                      train_dataset=DatasetTorch(train_enc, y_train.tolist()),\n",
        "                      eval_dataset=DatasetTorch(test_enc, y_test.tolist()),\n",
        "                      compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))})\n",
        "    trainer.train()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "azHbhzL8dUoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. EXECUTION OF THE ABLATION (MERGER) STUDY\n",
        "# MODEL A: Securities Only (WELFake + ISOT Merger)\n",
        "model_fus_tit, token_fus_tit = train_master(\n",
        "    train_sample[\"title\"], test_sample[\"title\"],\n",
        "    train_sample[\"label\"], test_sample[\"label\"], \"FUSION_TITLES\"\n",
        ")\n",
        "\n",
        "# Save locally\n",
        "model_fus_tit.save_pretrained('./model_fusion_titles')\n",
        "token_fus_tit.save_pretrained('./model_fusion_titles')\n",
        "\n",
        "# Connect and copy to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "# We copy it to the folder (using dirs_exist_ok=True in case you repeat the execution).\n",
        "shutil.copytree('./model_fusion_titles', '/content/drive/MyDrive/Project_IA/model_fusion_titles', dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "HSe27tSIdYwz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL B: Title + Text (Complete and Clean Merge)\n",
        "model_fus_full, token_fus_full = train_master(\n",
        "    train_sample[\"total\"], test_sample[\"total\"],\n",
        "    train_sample[\"label\"], test_sample[\"label\"], \"FUSION_FULL_TEXT\"\n",
        ")\n",
        "\n",
        "# Save locally\n",
        "model_fus_full.save_pretrained('./model_fusion_full')\n",
        "token_fus_full.save_pretrained('./model_fusion_full')"
      ],
      "metadata": {
        "id": "zuZd9FWWfHXl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Connect and copy to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "# We copy it into the folder.\n",
        "shutil.copytree('./model_fusion_full', '/content/drive/MyDrive/Project_IA/model_fusion_full', dirs_exist_ok=True)\n",
        "\n",
        "print(\"\\nâœ… Fusion ablation study completed and models exported!\")"
      ],
      "metadata": {
        "id": "aGskIia4fMgE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}