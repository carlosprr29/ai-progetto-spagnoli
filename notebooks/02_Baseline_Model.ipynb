{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosprr29/ai-progetto-spagnoli/blob/main/notebooks/02_Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Environment"
      ],
      "metadata": {
        "id": "htwLKrpg4YKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL-1SMSM4LAE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "##Installation and loading cell\n",
        "!pip install -q datasets pandas scikit-learn\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Download the WELFake dataset\n",
        "print(\"Downloading dataset...\")\n",
        "dataset_raw = load_dataset(\"davanstrien/WELFake\")\n",
        "df = pd.DataFrame(dataset_raw['train'])\n",
        "\n",
        "# Quick cleaning: remove rows that do not have text or a title\n",
        "df = df.dropna(subset=['title', 'text'])\n",
        "print(f\"✅ Dataset loaded: {len(df)} rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Split Cell\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We split the dataset (80% training, 20% testing)\n",
        "# 'stratify' ensures that there is the same proportion of real and fake news in both groups\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "print(f\"Data for training: {len(train_df)}\")\n",
        "print(f\"Data for evaluation: {len(test_df)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p2Qa5B7XQ-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Experiment 1: Baseline with Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# We define the training and assessment function.\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, experiment_name):\n",
        "    # 1. We create the model (TF-IDF + Logistic Regression)\n",
        "    # TF-IDF converts words into numbers according to their importance (max_features prevents it from being too heavy)\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # 2. We train\n",
        "    print(f\"\\n Training model for: {experiment_name}...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # 3. We predict\n",
        "    predictions = pipeline.predict(X_test)\n",
        "\n",
        "    # 4. Results\n",
        "    acc = accuracy_score(y_test, predictions)\n",
        "    print(f\"✅ Accuracy ({experiment_name}): {acc:.4f}\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "    return acc\n",
        "\n",
        "# --- PERFORMANCE OF THE ABLATION STUDY ---\n",
        "\n",
        "# CASE 1: Title only\n",
        "acc_titles = train_and_evaluate(\n",
        "      train_df['title'], test_df['title'],\n",
        "      train_df['label'], test_df['label'],\n",
        "      \"TITLES ONLY\"\n",
        ")\n",
        "\n",
        "# CASE 2: Title + Text (We created the 'total' column just before)\n",
        "train_df['total'] = train_df['title'] + \" \" + train_df['text']\n",
        "test_df['total'] = test_df['title'] + \" \" + test_df['text']\n",
        "\n",
        "acc_completo = train_and_evaluate(\n",
        "    train_df['total'], test_df['total'],\n",
        "    train_df['label'], test_df['label'],\n",
        "    \"FULL TEXT\"\n",
        ")"
      ],
      "metadata": {
        "id": "RmpFWfu3Rbsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline effectiveness:** 94.36% in full text is an extremely high score. This indicates that the WELFake dataset has very clear word patterns that make it easy to distinguish real news from fake news.\n",
        "\n",
        "**Ablation Study (Initial finding)**: There is a difference of 5 points (89% vs 94%).\n",
        "\n",
        "**Conclusion:** Although the headline alone is very informative, the body of the news article provides the necessary context to capture an extra 5% of cases that the headline does not reveal.\n",
        "\n",
        "**The challenge for BERT:** We have to see if BERT is capable of understanding more ambiguous news stories where the keywords are not so obvious.\n"
      ],
      "metadata": {
        "id": "zE16wY-TXM0f"
      }
    }
  ]
}